// server/controllers/layerData.controller.js - FIXED VERSION
const pool = require("../db");
const convertTcvn3ToUnicode = require("../utils/convertTcvn3ToUnicode");

/**
 * Memory cache (gi·ªØ nguy√™n ƒë·ªÉ backward compatibility)
 */
const cache = new Map();
const CACHE_TTL = 10 * 60 * 1000; // 10 ph√∫t

/**
 * Progress tracking cho real-time updates
 */
const progressTracking = new Map();

/**
 * ‚úÖ API ƒë·ªÉ l·∫•y progress real-time - FUNCTION B·ªä THI·∫æU
 */
exports.getProgress = async (req, res) => {
  try {
    const { layer } = req.params;
    const progress = progressTracking.get(layer) || { 
      current: 0, 
      total: 0, 
      percentage: 0, 
      stage: 'idle',
      timestamp: Date.now()
    };
    
    console.log(`üìä Progress request for ${layer}:`, progress);
    res.json(progress);
  } catch (error) {
    console.error(`‚ùå Error getting progress for ${layer}:`, error);
    res.status(500).json({ 
      error: "Error getting progress",
      details: error.message 
    });
  }
};

/**
 * H√†m helper ƒë·ªÉ c·∫≠p nh·∫≠t progress
 */
const updateProgress = (layerKey, current, total, stage = 'loading') => {
  const progress = {
    current,
    total,
    percentage: total > 0 ? Math.round((current / total) * 100) : 0,
    stage,
    timestamp: Date.now()
  };
  progressTracking.set(layerKey, progress);
  console.log(`üìä Progress ${layerKey}: ${current}/${total} (${progress.percentage}%)`);
};

/**
 * H√†m streaming v·ªõi progress tracking
 */
const streamLargeDatasetWithProgress = async (layerKey, tableName, query, simplifyTolerance = 0.00001, pageSize = 5000) => {
  const client = await pool.connect();
  
  try {
    // Initialize progress
    updateProgress(layerKey, 0, 0, 'initializing');
    
    // Get total count first
    const countQuery = `SELECT COUNT(*) as total FROM (${query.replace(/SELECT.*?FROM/, 'SELECT 1 FROM').replace(/ORDER BY.*$/, '')}) as count_query`;
    const countResult = await client.query(countQuery);
    const totalRecords = parseInt(countResult.rows[0].total);
    
    updateProgress(layerKey, 0, totalRecords, 'counting');
    
    console.log(`üîÑ Starting streaming for ${layerKey}: ${totalRecords} total records`);
    
    await client.query('BEGIN');
    await client.query('SET work_mem = "256MB"');
    await client.query('COMMIT');
    
    const allFeatures = [];
    let offset = 0;
    let hasMore = true;
    let totalLoaded = 0;
    
    while (hasMore) {
      const startTime = Date.now();
      
      // Update progress
      updateProgress(layerKey, totalLoaded, totalRecords, 'streaming');
      
      const paginatedQuery = query.replace(/ORDER BY.*?;?\s*$/, '') + 
        ` ORDER BY gid LIMIT ${pageSize} OFFSET ${offset}`;
      
      const result = await client.query(paginatedQuery);
      const loadTime = Date.now() - startTime;
      
      if (result.rows.length === 0) {
        hasMore = false;
        break;
      }
      
      // Process features
      for (const row of result.rows) {
        try {
          const feature = {
            type: "Feature",
            geometry: JSON.parse(row.geometry),
            properties: { ...row }
          };
          delete feature.properties.geometry;
          allFeatures.push(feature);
        } catch (err) {
          console.warn(`‚ö†Ô∏è Skipping invalid geometry for gid: ${row.gid}`);
        }
      }
      
      totalLoaded += result.rows.length;
      console.log(`‚úÖ Loaded ${result.rows.length} records in ${loadTime}ms (Total: ${totalLoaded}/${totalRecords})`);
      
      // Update progress
      updateProgress(layerKey, totalLoaded, totalRecords, 'processing');
      
      offset += pageSize;
      
      if (result.rows.length < pageSize) {
        hasMore = false;
      }
      
      if (offset > 1000000) {
        console.warn(`‚ö†Ô∏è Safety limit reached`);
        hasMore = false;
      }
    }
    
    // Final progress update
    updateProgress(layerKey, totalLoaded, totalRecords, 'completed');
    
    console.log(`üéâ Completed streaming ${totalLoaded} features for ${layerKey}`);
    
    return {
      type: "FeatureCollection",
      features: allFeatures,
      metadata: {
        total_features: totalLoaded,
        load_strategy: 'stream_with_progress',
        page_size: pageSize,
        build_time: Date.now() - (progressTracking.get(layerKey)?.timestamp || Date.now()),
        cache_saved: false
      }
    };
    
  } finally {
    client.release();
  }
};

/**
 * ‚úÖ L·∫•y th√¥ng tin t·ªïng quan v·ªÅ c√°c l·ªõp d·ªØ li·ªáu
 */
exports.getLayerInfo = async (req, res) => {
  try {
    const info = {};
    
    const tables = [
      { name: 'laocai_ranhgioihc', key: 'administrative' },
      { name: 'laocai_chuquanly', key: 'forest_management' },
      { name: 'laocai_nendiahinh', key: 'terrain' },
      { name: 'laocai_rg3lr', key: 'forest_types' },
      { name: 'mat_rung', key: 'deforestation_alerts' }
    ];

    for (const table of tables) {
      try {
        const result = await pool.query(`
          SELECT COUNT(*) as count,
                 ST_Extent(geom) as bbox
          FROM ${table.name} 
          WHERE ST_IsValid(geom)
        `);
        
        info[table.key] = {
          table_name: table.name,
          total_records: parseInt(result.rows[0].count),
          bbox: result.rows[0].bbox,
          available: true
        };
      } catch (err) {
        info[table.key] = {
          table_name: table.name,
          total_records: 0,
          bbox: null,
          available: false,
          error: err.message
        };
      }
    }

    console.log("üìä Layer data info:", info);
    res.json(info);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y th√¥ng tin layers:", err);
    res.status(500).json({ error: "L·ªói server khi l·∫•y th√¥ng tin layers" });
  }
};

/**
 * ‚úÖ L·∫•y d·ªØ li·ªáu l·ªõp ranh gi·ªõi h√†nh ch√≠nh
 */
exports.getAdministrativeBoundaries = async (req, res) => {
  const layerKey = 'administrative';
  
  try {
    console.log(`üì• Loading administrative boundaries from laocai_ranhgioihc`);
    
    const query = `
      SELECT 
        gid,
        huyen,
        xa,
        tieukhu,
        khoanh,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, 0.00001)) as geometry
      FROM laocai_ranhgioihc
      WHERE ST_IsValid(geom) AND geom IS NOT NULL
    `;

    const geojson = await streamLargeDatasetWithProgress(layerKey, 'laocai_ranhgioihc', query, 0.00001, 2000);
    
    // Chuy·ªÉn ƒë·ªïi TCVN3 sang Unicode v√† th√™m properties
    geojson.features = geojson.features.map(feature => ({
      ...feature,
      properties: {
        gid: feature.properties.gid,
        huyen: convertTcvn3ToUnicode(feature.properties.huyen || ""),
        xa: convertTcvn3ToUnicode(feature.properties.xa || ""),
        tieukhu: convertTcvn3ToUnicode(feature.properties.tieukhu || ""),
        khoanh: convertTcvn3ToUnicode(feature.properties.khoanh || ""),
        layer_type: 'administrative_boundary',
        boundary_level: getBoundaryLevel(feature.properties)
      }
    }));

    console.log(`‚úÖ Loaded ${geojson.features.length} administrative boundary features`);
    res.json(geojson);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y d·ªØ li·ªáu ranh gi·ªõi h√†nh ch√≠nh:", err);
    res.status(500).json({ error: "L·ªói server khi l·∫•y d·ªØ li·ªáu ranh gi·ªõi h√†nh ch√≠nh" });
  }
};

/**
 * ‚úÖ L·∫•y d·ªØ li·ªáu l·ªõp ch·ªß qu·∫£n l√Ω r·ª´ng
 */
exports.getForestManagement = async (req, res) => {
  const layerKey = 'forestManagement';
  
  try {
    console.log(`üì• Loading forest management data from laocai_chuquanly`);
    
    const query = `
      SELECT 
        gid,
        tt,
        chuquanly,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, 0.00001)) as geometry
      FROM laocai_chuquanly
      WHERE ST_IsValid(geom) AND geom IS NOT NULL
    `;

    const geojson = await streamLargeDatasetWithProgress(layerKey, 'laocai_chuquanly', query, 0.00001, 3000);

    // Chuy·ªÉn ƒë·ªïi TCVN3 sang Unicode
    geojson.features = geojson.features.map(feature => ({
      ...feature,
      properties: {
        gid: feature.properties.gid,
        tt: feature.properties.tt,
        chuquanly: convertTcvn3ToUnicode(feature.properties.chuquanly || ""),
        layer_type: 'forest_management'
      }
    }));

    console.log(`‚úÖ Loaded ${geojson.features.length} forest management features`);
    res.json(geojson);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y d·ªØ li·ªáu ch·ªß qu·∫£n l√Ω r·ª´ng:", err);
    res.status(500).json({ 
      error: "L·ªói server khi l·∫•y d·ªØ li·ªáu ch·ªß qu·∫£n l√Ω r·ª´ng",
      details: err.message
    });
  }
};

/**
 * ‚úÖ L·∫•y d·ªØ li·ªáu l·ªõp n·ªÅn ƒë·ªãa h√¨nh
 */
exports.getTerrainData = async (req, res) => {
  const layerKey = 'terrain';
  
  try {
    console.log(`üì• Loading terrain data from laocai_nendiahinh and laocai_nendiahinh_line`);
    
    // Query cho polygons
    const polygonQuery = `
      SELECT 
        gid,
        id,
        ma,
        ten,
        'terrain_polygon' as layer_type,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, 0.0001)) as geometry
      FROM laocai_nendiahinh
      WHERE ST_IsValid(geom) AND geom IS NOT NULL
    `;

    // Query cho lines  
    const lineQuery = `
      SELECT 
        gid,
        id,
        ma,
        ten,
        'terrain_line' as layer_type,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, 0.0001)) as geometry
      FROM laocai_nendiahinh_line
      WHERE ST_IsValid(geom) AND geom IS NOT NULL
    `;

    // Load c·∫£ hai lo·∫°i song song
    const [polygonData, lineData] = await Promise.all([
      streamLargeDatasetWithProgress(`${layerKey}_polygon`, 'laocai_nendiahinh', polygonQuery, 0.0001, 2000),
      streamLargeDatasetWithProgress(`${layerKey}_line`, 'laocai_nendiahinh_line', lineQuery, 0.0001, 2000)
    ]);

    // G·ªôp features v√† convert TCVN3
    const allFeatures = [
      ...polygonData.features.map(feature => ({
        ...feature,
        properties: {
          gid: feature.properties.gid,
          id: feature.properties.id,
          ma: feature.properties.ma,
          ten: convertTcvn3ToUnicode(feature.properties.ten || ""),
          layer_type: 'terrain_polygon',
          feature_type: getFeatureType(feature.properties.ten)
        }
      })),
      ...lineData.features.map(feature => ({
        ...feature,
        properties: {
          gid: feature.properties.gid,
          id: feature.properties.id,
          ma: feature.properties.ma,
          ten: convertTcvn3ToUnicode(feature.properties.ten || ""),
          layer_type: 'terrain_line',
          feature_type: getFeatureType(feature.properties.ten)
        }
      }))
    ];

    const geojson = {
      type: "FeatureCollection",
      features: allFeatures,
      metadata: {
        polygon_count: polygonData.features.length,
        line_count: lineData.features.length,
        total_features: allFeatures.length,
        load_strategy: 'parallel_stream'
      }
    };
    
    console.log(`‚úÖ Loaded ${geojson.features.length} terrain features (${polygonData.features.length} polygons, ${lineData.features.length} lines)`);
    res.json(geojson);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y d·ªØ li·ªáu n·ªÅn ƒë·ªãa h√¨nh:", err);
    res.status(500).json({ 
      error: "L·ªói server khi l·∫•y d·ªØ li·ªáu n·ªÅn ƒë·ªãa h√¨nh",
      details: err.message
    });
  }
};

/**
 * ‚úÖ L·∫•y d·ªØ li·ªáu l·ªõp c√°c lo·∫°i r·ª´ng - OPTIMIZED
 */
exports.getForestTypes = async (req, res) => {
  const layerKey = 'forestTypes';
  
  try {
    console.log(`üì• Loading forest types data from laocai_rg3lr based on LDLR column - OPTIMIZED`);
    
    // ƒê·∫øm t·ªïng s·ªë records tr∆∞·ªõc
    const countResult = await pool.query(`
      SELECT COUNT(*) as total 
      FROM laocai_rg3lr 
      WHERE ST_IsValid(geom) AND geom IS NOT NULL AND ldlr IS NOT NULL AND TRIM(ldlr) != ''
    `);
    
    const totalRecords = parseInt(countResult.rows[0].total);
    console.log(`üìä Total forest type records: ${totalRecords}`);

    // V·ªõi dataset l·ªõn, tƒÉng tolerance v√† gi·∫£m detail
    const tolerance = totalRecords > 100000 ? 0.001 : 0.0001;
    const pageSize = 3000;

    const query = `
      SELECT 
        gid,
        xa,
        tk,
        khoanh,
        lo,
        dtich,
        ldlr,
        malr3,
        churung,
        tinh,
        huyen,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, ${tolerance})) as geometry
      FROM laocai_rg3lr
      WHERE ST_IsValid(geom) 
        AND geom IS NOT NULL
        AND ldlr IS NOT NULL 
        AND TRIM(ldlr) != ''
    `;

    console.log(`üöÄ Starting optimized streaming with tolerance: ${tolerance}, pageSize: ${pageSize}`);
    
    const geojson = await streamLargeDatasetWithProgress(layerKey, 'laocai_rg3lr', query, tolerance, pageSize);

    // Chuy·ªÉn ƒë·ªïi TCVN3 v√† th√™m metadata
    geojson.features = geojson.features.map(feature => ({
      ...feature,
      properties: {
        gid: feature.properties.gid,
        xa: convertTcvn3ToUnicode(feature.properties.xa || ""),
        tk: feature.properties.tk,
        khoanh: feature.properties.khoanh,
        lo: feature.properties.lo,
        dtich: feature.properties.dtich,
        ldlr: feature.properties.ldlr,
        malr3: feature.properties.malr3,
        churung: convertTcvn3ToUnicode(feature.properties.churung || ""),
        tinh: convertTcvn3ToUnicode(feature.properties.tinh || ""),
        huyen: convertTcvn3ToUnicode(feature.properties.huyen || ""),
        layer_type: 'forest_types_ldlr',
        forest_function: getForestFunction(feature.properties.ldlr),
        ldlr_code: feature.properties.ldlr,
        ldlr_category: getLdlrCategory(feature.properties.ldlr)
      }
    }));

    // T√≠nh to√°n th·ªëng k√™
    const typeStats = {};
    const categoryStats = {};
    
    geojson.features.forEach(feature => {
      const forestFunction = feature.properties.forest_function;
      const category = feature.properties.ldlr_category;
      
      typeStats[forestFunction] = (typeStats[forestFunction] || 0) + 1;
      categoryStats[category] = (categoryStats[category] || 0) + 1;
    });
    
    // Th√™m metadata
    geojson.forestTypes = Object.keys(typeStats).map(type => ({
      name: type,
      count: typeStats[type]
    })).sort((a, b) => b.count - a.count);

    geojson.forestCategories = Object.keys(categoryStats).map(category => ({
      name: category,
      count: categoryStats[category]
    })).sort((a, b) => b.count - a.count);

    console.log(`üìä Th·ªëng k√™ c√°c lo·∫°i r·ª´ng theo LDLR:`, typeStats);
    console.log(`üìä Th·ªëng k√™ theo nh√≥m:`, categoryStats);
    console.log(`‚úÖ Loaded ${geojson.features.length} forest features with ${Object.keys(typeStats).length} different types`);
    
    res.json(geojson);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y d·ªØ li·ªáu c√°c lo·∫°i r·ª´ng theo LDLR:", err);
    res.status(500).json({ 
      error: "L·ªói server khi l·∫•y d·ªØ li·ªáu c√°c lo·∫°i r·ª´ng theo LDLR",
      details: err.message
    });
  }
};

/**
 * ‚úÖ L·∫•y d·ªØ li·ªáu l·ªõp d·ª± b√°o m·∫•t r·ª´ng m·ªõi nh·∫•t
 */
exports.getDeforestationAlerts = async (req, res) => {
  const days = parseInt(req.query.days) || 365;
  const layerKey = `deforestationAlerts`;
  
  try {
    console.log(`üì• Loading deforestation alerts from mat_rung`);
    
    const query = `
      SELECT 
        gid,
        start_dau,
        end_sau,
        area,
        mahuyen,
        detection_status,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, 0.00001)) as geometry
      FROM mat_rung
      WHERE ST_IsValid(geom)
        AND end_sau::date >= CURRENT_DATE - INTERVAL '${days} days'
    `;

    const geojson = await streamLargeDatasetWithProgress(layerKey, 'mat_rung', query, 0.00001, 2000);

    // Th√™m properties cho deforestation alerts
    geojson.features = geojson.features.map(feature => ({
      ...feature,
      properties: {
        gid: feature.properties.gid,
        start_dau: feature.properties.start_dau,
        end_sau: feature.properties.end_sau,
        area: feature.properties.area,
        area_ha: Math.round((feature.properties.area / 10000) * 100) / 100,
        mahuyen: feature.properties.mahuyen,
        layer_type: 'deforestation_alert',
        alert_level: getAlertLevel(feature.properties.end_sau),
        days_since: getDaysSince(feature.properties.end_sau),
        detection_status: feature.properties.detection_status || 'Ch∆∞a x√°c minh'
      }
    }));

    console.log(`‚úÖ Loaded ${geojson.features.length} deforestation alert features from last ${days} days`);
    res.json(geojson);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y d·ªØ li·ªáu d·ª± b√°o m·∫•t r·ª´ng:", err);
    res.status(500).json({ error: "L·ªói server khi l·∫•y d·ªØ li·ªáu d·ª± b√°o m·∫•t r·ª´ng" });
  }
};

/**
 * ‚úÖ L·∫•y d·ªØ li·ªáu l·ªõp hi·ªán tr·∫°ng r·ª´ng (legacy endpoint)
 */
exports.getForestStatus = async (req, res) => {
  try {
    console.log(`üì• Loading forest status data from tlaocai_tkk_3lr_cru`);
    
    const query = `
      SELECT 
        gid,
        huyen,
        xa,
        tk,
        khoanh,
        lo,
        thuad,
        dtich,
        ldlr,
        churung,
        ST_AsGeoJSON(ST_SimplifyPreserveTopology(geom, 0.0001)) as geometry
      FROM tlaocai_tkk_3lr_cru
      WHERE ST_IsValid(geom) AND geom IS NOT NULL
    `;

    const geojson = await streamLargeDatasetWithProgress('forestStatus', 'tlaocai_tkk_3lr_cru', query, 0.0001, 3000);

    // Chuy·ªÉn ƒë·ªïi TCVN3 sang Unicode
    geojson.features = geojson.features.map(feature => ({
      ...feature,
      properties: {
        gid: feature.properties.gid,
        huyen: convertTcvn3ToUnicode(feature.properties.huyen || ""),
        xa: convertTcvn3ToUnicode(feature.properties.xa || ""),
        tk: feature.properties.tk,
        khoanh: feature.properties.khoanh,
        lo: feature.properties.lo,
        thuad: feature.properties.thuad,
        dtich: feature.properties.dtich,
        ldlr: convertTcvn3ToUnicode(feature.properties.ldlr || ""),
        churung: convertTcvn3ToUnicode(feature.properties.churung || ""),
        layer_type: 'current_forest_status',
        area_ha: Math.round((feature.properties.dtich || 0) * 100) / 100
      }
    }));

    console.log(`‚úÖ Loaded ${geojson.features.length} forest status features`);
    res.json(geojson);
  } catch (err) {
    console.error("‚ùå L·ªói l·∫•y d·ªØ li·ªáu hi·ªán tr·∫°ng r·ª´ng:", err);
    res.status(500).json({ error: "L·ªói server khi l·∫•y d·ªØ li·ªáu hi·ªán tr·∫°ng r·ª´ng" });
  }
};

/**
 * ‚úÖ API ƒë·ªÉ clear cache (memory only - gi·ªØ cho backward compatibility)
 */
exports.clearCache = async (req, res) => {
  try {
    cache.clear();
    progressTracking.clear();
    console.log("üóëÔ∏è Cache cleared");
    res.json({ success: true, message: "Cache ƒë√£ ƒë∆∞·ª£c x√≥a" });
  } catch (err) {
    res.status(500).json({ error: "L·ªói khi x√≥a cache" });
  }
};

/**
 * ‚úÖ API ƒë·ªÉ xem cache status (memory only)
 */
exports.getCacheStatus = async (req, res) => {
  try {
    const cacheInfo = [];
    for (const [key, value] of cache.entries()) {
      cacheInfo.push({
        key,
        size: JSON.stringify(value.data).length,
        age: Date.now() - value.timestamp,
        expires_in: CACHE_TTL - (Date.now() - value.timestamp)
      });
    }
    
    res.json({
      cache_count: cache.size,
      cache_ttl: CACHE_TTL,
      cache_entries: cacheInfo,
      progress_tracking: Object.fromEntries(progressTracking)
    });
  } catch (err) {
    res.status(500).json({ error: "L·ªói khi l·∫•y th√¥ng tin cache" });
  }
};

/**
 * ‚úÖ MOCK Server cache management APIs (ƒë·ªÉ tr√°nh l·ªói routes)
 */
exports.getServerCacheStatus = async (req, res) => {
  try {
    // Mock response - c√≥ th·ªÉ implement persistent cache sau
    const status = {
      memory_cache_count: cache.size,
      file_cache_count: 0,
      cached_layers: [],
      total_cache_size: 0
    };
    
    console.log("üìä Server cache status (mock):", status);
    res.json(status);
  } catch (err) {
    console.error("‚ùå Error getting server cache status:", err);
    res.status(500).json({ error: "Error getting server cache status" });
  }
};

exports.clearServerCache = async (req, res) => {
  try {
    // Mock implementation - clear memory cache
    cache.clear();
    progressTracking.clear();
    
    console.log("üóëÔ∏è Server cache cleared (mock)");
    res.json({ success: true, message: "Server cache cleared" });
  } catch (err) {
    console.error("‚ùå Error clearing server cache:", err);
    res.status(500).json({ error: "Error clearing server cache" });
  }
};

exports.rebuildServerCache = async (req, res) => {
  try {
    // Mock implementation  
    cache.clear();
    progressTracking.clear();
    
    console.log("üîÑ Server cache rebuild initiated (mock)");
    res.json({ success: true, message: "Server cache rebuild initiated" });
  } catch (err) {
    console.error("‚ùå Error rebuilding server cache:", err);
    res.status(500).json({ error: "Error rebuilding server cache" });
  }
};

// ============================================
// HELPER FUNCTIONS
// ============================================

function getBoundaryLevel(props) {
  if (props.khoanh && props.khoanh.trim() !== '') return 'khoanh';
  if (props.tieukhu && props.tieukhu.trim() !== '') return 'tieukhu';
  if (props.xa && props.xa.trim() !== '') return 'xa';
  if (props.huyen && props.huyen.trim() !== '') return 'huyen';
  return 'unknown';
}

function getFeatureType(ten) {
  if (!ten) return 'terrain';
  
  const tenLower = ten.toLowerCase();
  
  if (tenLower.includes('s√¥ng') || tenLower.includes('su·ªëi') || tenLower.includes('k√™nh')) {
    return 'waterway';
  }
  if (tenLower.includes('th·ªßy') || tenLower.includes('c·∫£ng')) {
    return 'water_transport';
  }
  if (tenLower.includes('ƒë∆∞·ªùng') || tenLower.includes('qu·ªëc l·ªô') || tenLower.includes('t·ªânh l·ªô')) {
    return 'road';
  }
  
  return 'terrain';
}

function getForestFunction(ldlr) {
  if (!ldlr) return 'Kh√¥ng x√°c ƒë·ªãnh';
  
  const ldlrUpper = ldlr.trim().toUpperCase();
  const functionMap = {
    'RTG': 'R·ª´ng t·ª± nhi√™n gi√†u',
    'RTN': 'R·ª´ng t·ª± nhi√™n ngh√®o',
    'RTTN': 'R·ª´ng tr·ªìng t·ª± nhi√™n',
    'RTK': 'R·ª´ng tr·ªìng kh√°c',
    'RTCD': 'R·ª´ng tr·ªìng c√¢y d∆∞·ª£c li·ªáu',
    'TXN': 'Tr·ªìng xen n∆∞∆°ng',
    'TXP': 'Tr·ªìng xen ph·ª•',
    'TXK': 'Tr·ªìng xen kh√°c',
    'TXDN': 'Tr·ªìng xen ƒë·∫∑c n√¥ng',
    'TNK': 'Tr·ªìng n∆∞∆°ng kh√°c',
    'DT1': 'ƒê·∫•t tr·ªëng lo·∫°i 1',
    'DT2': 'ƒê·∫•t tr·ªëng lo·∫°i 2',
    'DTR': 'ƒê·∫•t tr·ªëng r·ª´ng',
    'DNN': 'ƒê·∫•t n√¥ng nghi·ªáp',
    'HG1': 'H·ªón giao lo·∫°i 1',
    'HG2': 'H·ªón giao lo·∫°i 2'
  };
  
  return functionMap[ldlrUpper] || ldlr;
}

function getLdlrCategory(ldlr) {
  if (!ldlr) return 'Kh√°c';
  
  const ldlrUpper = ldlr.trim().toUpperCase();
  
  if (['RTG', 'RTN', 'RTTN'].includes(ldlrUpper)) return 'R·ª´ng t·ª± nhi√™n';
  if (['RTK', 'RTCD'].includes(ldlrUpper)) return 'R·ª´ng tr·ªìng';
  if (['TXN', 'TXP', 'TXK', 'TXDN', 'TNK'].includes(ldlrUpper)) return 'ƒê·∫•t tr·ªìng c√¢y l√¢m nghi·ªáp';
  if (['DT1', 'DT2', 'DTR'].includes(ldlrUpper)) return 'ƒê·∫•t tr·ªëng';
  if (ldlrUpper === 'DNN') return 'ƒê·∫•t n√¥ng nghi·ªáp';
  if (['HG1', 'HG2'].includes(ldlrUpper)) return 'H·ªón giao';
  
  return 'Kh√°c';
}

function getAlertLevel(endDate) {
  const daysSince = getDaysSince(endDate);
  if (daysSince <= 7) return 'critical';
  if (daysSince <= 15) return 'high';
  if (daysSince <= 30) return 'medium';
  return 'low';
}

function getDaysSince(endDate) {
  const today = new Date();
  const end = new Date(endDate);
  return Math.floor((today - end) / (1000 * 60 * 60 * 24));
}